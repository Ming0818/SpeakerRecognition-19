{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# $Id$\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy import interp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample class\n",
    "class VoiceSample:\n",
    "    path = ''\n",
    "    personId = -1\n",
    "    feature = []\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.personId = getPerson(path)\n",
    "        self.getAvgMFCC()\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.path\n",
    "    \n",
    "    def getAvgMFCC(self):\n",
    "        y, sr = librosa.load(self.path,sr=None)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr,n_mfcc=13)\n",
    "        self.feature = np.append(np.average(mfccs, 1), np.std(mfccs, 1))\n",
    "        \n",
    "def getTrainFeature(data):\n",
    "    return np.asarray([data[i].feature for i in xrange(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"data\", \"rb\"))\n",
    "\n",
    "[X_train, X_test, y_train, y_test] = data\n",
    "X_train = getTrainFeature(X_train)\n",
    "X_test = getTrainFeature(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianMixture(n_components=classes, init_params='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.means_init  = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                  for i in xrange(classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6979490366687383\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7967681789931634\n"
     ]
    }
   ],
   "source": [
    "clf =svm.SVC(kernel = \"rbf\", gamma=0.0008)\n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('gmm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0008, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new gmm\n",
    "clf_gmm = GaussianMixture(n_components=classes, init_params='random')\n",
    "clf_gmm.means_init  = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                  for i in xrange(classes)])\n",
    "clf_svm = svm.SVC(kernel = \"rbf\", gamma=0.001, probability = True)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('gmm', clf), ('svm', clf_svm)], voting='soft')\n",
    "# eclf = VotingClassifier(estimators=[('gmm', classifier), ('svm', clf)], voting='hard')\n",
    "eclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-e7cf7cfa5723>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'soft'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[0mmaj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'hard' voting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    239\u001b[0m                                  \" voting=%r\" % self.voting)\n\u001b[0;32m    240\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         avg = np.average(self._collect_probas(X), axis=0,\n\u001b[0m\u001b[0;32m    242\u001b[0m                          weights=self._weights_not_none)\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36m_collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \"\"\"\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.pyc\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[0;32m    558\u001b[0m                                  \" probability=False\")\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'c_svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nu_svc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "y_pred = eclf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up the dataset into non-overlapping training (75%) and testing\n",
    "# (25%) sets.\n",
    "skf = StratifiedKFold(iris.target, n_folds=4)\n",
    "# Only take the first fold.\n",
    "train_index, test_index = next(iter(skf))\n",
    "\n",
    "\n",
    "X_train = iris.data[train_index]\n",
    "y_train = iris.target[train_index]\n",
    "X_test = iris.data[test_index]\n",
    "y_test = iris.target[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# Try GMMs using different types of covariances.\n",
    "classifiers = dict((covar_type, GMM(n_components=n_classes,\n",
    "                    covariance_type=covar_type, init_params='wc', n_iter=20))\n",
    "                   for covar_type in ['spherical', 'diag', 'tied', 'full'])\n",
    "\n",
    "n_classifiers = len(classifiers)\n",
    "\n",
    "plt.figure(figsize=(3 * n_classifiers / 2, 6))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "\n",
    "for index, (name, classifier) in enumerate(classifiers.items()):\n",
    "    # Since we have class labels for the training data, we can\n",
    "    # initialize the GMM parameters in a supervised manner.\n",
    "    classifier.means_ = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                  for i in xrange(n_classes)])\n",
    "\n",
    "    # Train the other parameters using the EM algorithm.\n",
    "    classifier.fit(X_train)\n",
    "\n",
    "    h = plt.subplot(2, n_classifiers / 2, index + 1)\n",
    "    make_ellipses(classifier, h)\n",
    "\n",
    "    for n, color in enumerate('rgb'):\n",
    "        data = iris.data[iris.target == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], 0.8, color=color,\n",
    "                    label=iris.target_names[n])\n",
    "    # Plot the test data with crosses\n",
    "    for n, color in enumerate('rgb'):\n",
    "        data = X_test[y_test == n]\n",
    "        plt.plot(data[:, 0], data[:, 1], 'x', color=color)\n",
    "\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n",
    "    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n",
    "    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n",
    "             transform=h.transAxes)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(loc='lower right', prop=dict(size=12))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
